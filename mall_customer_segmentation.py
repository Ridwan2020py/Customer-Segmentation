# -*- coding: utf-8 -*-
"""Mall Customer Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IUFKtzy0zTG1FUKOwMOw3TSubu40imvk

### Import libraries and Load data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
df = pd.read_csv('/content/sample_data/Mall_Customers.csv')
df.head()

"""### Exploratory Data Analysis"""

# Basic information about the dataset
print("Dataset Shape:", df.shape)
print("\nDataset Info:")
print(df.info())
print("\nFirst 5 rows:")
print(df.head())

# Statistical summary
print("\nStatistical Summary:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

"""### Data Preprocessing and Feature Selection"""

# Select features for clustering (Income and Spending Score)
features = df[['Annual Income (k$)', 'Spending Score (1-100)']]

# Check for outliers using box plots
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.boxplot(features['Annual Income (k$)'])
plt.title('Annual Income Distribution')
plt.ylabel('Income (k$)')

plt.subplot(1, 2, 2)
plt.boxplot(features['Spending Score (1-100)'])
plt.title('Spending Score Distribution')
plt.ylabel('Spending Score')
plt.tight_layout()
plt.show()

"""#### Feature scaling"""

# Scale the features for better clustering performance
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Convert back to DataFrame for easier handling
features_scaled_df = pd.DataFrame(features_scaled,
                                columns=['Annual Income (Scaled)', 'Spending Score (Scaled)'])

"""### Determine Optimal Number of Clusters (Elbow Method)"""

# Calculate WCSS (Within-Cluster Sum of Squares) for different k values
wcss = []
k_range = range(1, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(features_scaled)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Curve
plt.figure(figsize=(10, 6))
plt.plot(k_range, wcss, 'bo-', linewidth=2, markersize=8)
plt.title('Elbow Method for Optimal k', fontsize=16)
plt.xlabel('Number of Clusters (k)', fontsize=14)
plt.ylabel('WCSS (Within-Cluster Sum of Squares)', fontsize=14)
plt.grid(True, alpha=0.3)
plt.show()

# Calculate silhouette scores
silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(features_scaled)
    score = silhouette_score(features_scaled, labels)
    silhouette_scores.append(score)
    print(f"k={k}: Silhouette Score = {score:.3f}")

"""### Apply k-means clustering"""

# Apply K-Means with optimal number of clusters (typically 5)
optimal_k = 5
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(features_scaled)

# Add cluster labels to the original dataframe
df['Cluster'] = cluster_labels

# Get cluster centers (in original scale)
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
cluster_centers_df = pd.DataFrame(cluster_centers,
                                columns=['Income_Center', 'Spending_Center'])
print("Cluster Centers:")
print(cluster_centers_df)

"""### Visualize clusters"""

# Create scatter plot of clusters
plt.figure(figsize=(14, 10))

# Plot 1: Original scale
plt.subplot(2, 2, 1)
colors = ['red', 'blue', 'green', 'purple', 'orange']
for i in range(optimal_k):
    cluster_data = df[df['Cluster'] == i]
    plt.scatter(cluster_data['Annual Income (k$)'],
               cluster_data['Spending Score (1-100)'],
               c=colors[i], label=f'Cluster {i}', alpha=0.7, s=50)

# Plot cluster centers
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1],
           c='black', marker='X', s=200, label='Centroids')
plt.title('Customer Segments (Original Scale)', fontsize=14)
plt.xlabel('Annual Income (k$)', fontsize=12)
plt.ylabel('Spending Score (1-100)', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 2: Scaled features
plt.subplot(2, 2, 2)
for i in range(optimal_k):
    cluster_mask = cluster_labels == i
    plt.scatter(features_scaled[cluster_mask, 0],
               features_scaled[cluster_mask, 1],
               c=colors[i], label=f'Cluster {i}', alpha=0.7, s=50)

plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
           c='black', marker='X', s=200, label='Centroids')
plt.title('Customer Segments (Scaled Features)', fontsize=14)
plt.xlabel('Annual Income (Scaled)', fontsize=12)
plt.ylabel('Spending Score (Scaled)', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)

# Plot 3: Cluster size distribution
plt.subplot(2, 2, 3)
cluster_counts = df['Cluster'].value_counts().sort_index()
plt.bar(cluster_counts.index, cluster_counts.values, color=colors[:len(cluster_counts)])
plt.title('Cluster Size Distribution', fontsize=14)
plt.xlabel('Cluster', fontsize=12)
plt.ylabel('Number of Customers', fontsize=12)
plt.grid(True, alpha=0.3)

# Plot 4: Average spending by cluster
plt.subplot(2, 2, 4)
avg_spending = df.groupby('Cluster')['Spending Score (1-100)'].mean()
plt.bar(avg_spending.index, avg_spending.values, color=colors[:len(avg_spending)])
plt.title('Average Spending Score by Cluster', fontsize=14)
plt.xlabel('Cluster', fontsize=12)
plt.ylabel('Average Spending Score', fontsize=12)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

"""### Analyze cluster characteristics"""

# Detailed cluster analysis
print("=== CLUSTER ANALYSIS ===\n")

for i in range(optimal_k):
    cluster_data = df[df['Cluster'] == i]
    print(f"CLUSTER {i} CHARACTERISTICS:")
    print(f"Size: {len(cluster_data)} customers ({len(cluster_data)/len(df)*100:.1f}%)")
    print(f"Average Income: ${cluster_data['Annual Income (k$)'].mean():.1f}k")
    print(f"Average Spending Score: {cluster_data['Spending Score (1-100)'].mean():.1f}")
    print(f"Income Range: ${cluster_data['Annual Income (k$)'].min():.0f}k - ${cluster_data['Annual Income (k$)'].max():.0f}k")
    print(f"Spending Range: {cluster_data['Spending Score (1-100)'].min():.0f} - {cluster_data['Spending Score (1-100)'].max():.0f}")

    # Customer interpretation
    avg_income = cluster_data['Annual Income (k$)'].mean()
    avg_spending = cluster_data['Spending Score (1-100)'].mean()

    if avg_income > 60 and avg_spending > 60:
        interpretation = "High Value Customers (High Income, High Spending)"
    elif avg_income < 40 and avg_spending > 60:
        interpretation = "Young/Impulsive Spenders (Low Income, High Spending)"
    elif avg_income > 60 and avg_spending < 40:
        interpretation = "Conservative High Earners (High Income, Low Spending)"
    elif avg_income < 40 and avg_spending < 40:
        interpretation = "Budget Conscious (Low Income, Low Spending)"
    else:
        interpretation = "Average Customers (Moderate Income and Spending)"

    print(f"Interpretation: {interpretation}")
    print("-" * 50)

"""### DBSCAN Clustering"""

# Apply DBSCAN clustering as an alternative
from sklearn.cluster import DBSCAN

# Try different eps and min_samples values
eps_values = [0.3, 0.5, 0.7]
min_samples_values = [3, 5, 7]

best_score = -1
best_params = {}

print("=== DBSCAN Parameter Tuning ===")
for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        dbscan_labels = dbscan.fit_predict(features_scaled)

        # Only calculate silhouette score if we have more than 1 cluster
        n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
        if n_clusters > 1:
            score = silhouette_score(features_scaled, dbscan_labels)
            print(f"eps={eps}, min_samples={min_samples}: {n_clusters} clusters, Silhouette={score:.3f}")

            if score > best_score:
                best_score = score
                best_params = {'eps': eps, 'min_samples': min_samples}

# Apply DBSCAN with best parameters
if best_params:
    dbscan_best = DBSCAN(**best_params)
    dbscan_labels = dbscan_best.fit_predict(features_scaled)
    df['DBSCAN_Cluster'] = dbscan_labels

    print(f"\nBest DBSCAN parameters: {best_params}")
    print(f"Number of clusters: {len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)}")
    print(f"Number of noise points: {list(dbscan_labels).count(-1)}")

"""### Advanced Analysis

"""

# Compare average spending per cluster
cluster_analysis = df.groupby('Cluster').agg({
    'Annual Income (k$)': ['mean', 'std', 'min', 'max'],
    'Spending Score (1-100)': ['mean', 'std', 'min', 'max'],
    'CustomerID': 'count'
}).round(2)

cluster_analysis.columns = ['_'.join(col).strip() for col in cluster_analysis.columns]
cluster_analysis = cluster_analysis.rename(columns={'CustomerID_count': 'Customer_Count'})

print("=== DETAILED CLUSTER STATISTICS ===")
print(cluster_analysis)

# Save results to CSV
df.to_csv('customer_segments_results.csv', index=False)
cluster_analysis.to_csv('cluster_statistics.csv')

print("\nResults saved to:")
print("- customer_segments_results.csv (individual customer data with cluster assignments)")
print("- cluster_statistics.csv (cluster summary statistics)")

